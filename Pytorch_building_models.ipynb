{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNJawghys7KEjlVESz0OiPT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Peter-obi/protein-structure-prediction-from-scratch/blob/main/Pytorch_building_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zKxRSiGhvHnh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TinyModel(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(TinyModel, self).__init__()\n",
        "\n",
        "    self.linear1 = nn.Linear(100, 200)\n",
        "    self.activation = nn.ReLU()\n",
        "    self.linear2 = nn.Linear(200, 10)\n",
        "    self.softmax = nn.Softmax()\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.linear1(x)\n",
        "    x = self.activation(x)\n",
        "    x = self.linear2(x)\n",
        "    x = self.softmax(x)\n",
        "    return x\n",
        "\n",
        "tinymodel = TinyModel()\n",
        "\n",
        "print('The Model')\n",
        "print(tinymodel)\n",
        "\n",
        "print('\\n\\nJust one layer:')\n",
        "print(tinymodel.linear2)\n",
        "\n",
        "print('\\n\\nModel params:')\n",
        "for params in tinymodel.parameters():\n",
        "  print(params)\n",
        "\n",
        "print('\\n\\nLayer params:')\n",
        "for params in tinymodel.linear2.parameters():\n",
        "  print(params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vs6K9NgNx5z-",
        "outputId": "776cfa51-ce71-49e0-ffae-522d712cb1d7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Model\n",
            "TinyModel(\n",
            "  (linear1): Linear(in_features=100, out_features=200, bias=True)\n",
            "  (activation): ReLU()\n",
            "  (linear2): Linear(in_features=200, out_features=10, bias=True)\n",
            "  (softmax): Softmax(dim=None)\n",
            ")\n",
            "\n",
            "\n",
            "Just one layer:\n",
            "Linear(in_features=200, out_features=10, bias=True)\n",
            "\n",
            "\n",
            "Model params:\n",
            "Parameter containing:\n",
            "tensor([[-0.0211, -0.0102, -0.0075,  ..., -0.0109, -0.0614, -0.0344],\n",
            "        [ 0.0350,  0.0118, -0.0243,  ..., -0.0956,  0.0264, -0.0084],\n",
            "        [-0.0333, -0.0483, -0.0951,  ..., -0.0419,  0.0713,  0.0800],\n",
            "        ...,\n",
            "        [-0.0763, -0.0199, -0.0464,  ..., -0.0509,  0.0826, -0.0525],\n",
            "        [ 0.0015,  0.0134,  0.0997,  ...,  0.0191, -0.0607,  0.0672],\n",
            "        [ 0.0191,  0.0152,  0.0434,  ..., -0.0148,  0.0624, -0.0923]],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 0.0970,  0.0055,  0.0021, -0.0122,  0.0728, -0.0259, -0.0649,  0.0939,\n",
            "        -0.0282, -0.0459, -0.0978,  0.0874, -0.0855, -0.0725, -0.0211,  0.0754,\n",
            "        -0.0372, -0.0453, -0.0860,  0.0523,  0.0975,  0.0809, -0.0421,  0.0911,\n",
            "         0.0244, -0.0695,  0.0107, -0.0368,  0.0916, -0.0551,  0.0154, -0.0100,\n",
            "         0.0812, -0.0705, -0.0850,  0.0210, -0.0575, -0.0120,  0.0884,  0.0785,\n",
            "        -0.0236, -0.0197, -0.0918,  0.0786,  0.0591,  0.0440,  0.0312,  0.0379,\n",
            "         0.0798,  0.0608,  0.0308, -0.0406, -0.0731, -0.0245,  0.0200,  0.0715,\n",
            "         0.0391,  0.0346, -0.0011,  0.0982,  0.0325, -0.0597,  0.0408, -0.0835,\n",
            "        -0.0192,  0.0404, -0.0960, -0.0021,  0.0510,  0.0814, -0.0624,  0.0149,\n",
            "        -0.0072,  0.0701, -0.0425,  0.0531, -0.0431,  0.0270,  0.0254,  0.0636,\n",
            "         0.0621,  0.0458,  0.0912,  0.0842, -0.0225,  0.0490,  0.0006,  0.0842,\n",
            "        -0.0165, -0.0460, -0.0277,  0.0185,  0.0504, -0.0944, -0.0010,  0.0356,\n",
            "         0.0147, -0.0173,  0.0713, -0.0489,  0.0747, -0.0698,  0.0749, -0.0666,\n",
            "        -0.0783,  0.0250,  0.0970, -0.0441, -0.0153,  0.0838, -0.0237, -0.0843,\n",
            "        -0.0896,  0.0445,  0.0070, -0.0013,  0.0878,  0.0293,  0.0655,  0.0373,\n",
            "         0.0786, -0.0475,  0.0843,  0.0842, -0.0878,  0.0957,  0.0961,  0.0488,\n",
            "        -0.0332, -0.0315,  0.0795,  0.0385, -0.0262, -0.0166, -0.0354,  0.0930,\n",
            "         0.0608,  0.0248, -0.0288,  0.0151,  0.0227, -0.0057, -0.0601, -0.0215,\n",
            "        -0.0579,  0.0895, -0.0850, -0.0407,  0.0264,  0.0507, -0.0613, -0.0024,\n",
            "        -0.0457,  0.0694, -0.0821,  0.0822, -0.0666, -0.0687,  0.0212, -0.0201,\n",
            "        -0.0685, -0.0430,  0.0663,  0.0821, -0.0659,  0.0047,  0.0227, -0.0223,\n",
            "        -0.0622, -0.0742, -0.0001, -0.0123,  0.0691, -0.0236,  0.0087, -0.0305,\n",
            "         0.0776,  0.0062, -0.0736, -0.0822,  0.0941,  0.0101,  0.0328,  0.0502,\n",
            "        -0.0560, -0.0607, -0.0577, -0.0348,  0.0141,  0.0551,  0.0925, -0.0744,\n",
            "        -0.0061, -0.0753,  0.0596,  0.0810, -0.0048,  0.0909,  0.0593,  0.0465],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[-0.0170,  0.0628,  0.0114,  ...,  0.0605,  0.0552,  0.0457],\n",
            "        [ 0.0609, -0.0412, -0.0646,  ...,  0.0143,  0.0189,  0.0413],\n",
            "        [-0.0168,  0.0396, -0.0511,  ...,  0.0619,  0.0102,  0.0025],\n",
            "        ...,\n",
            "        [-0.0624, -0.0488, -0.0495,  ..., -0.0153,  0.0269,  0.0098],\n",
            "        [-0.0458,  0.0182,  0.0376,  ...,  0.0615,  0.0233,  0.0126],\n",
            "        [-0.0685,  0.0475,  0.0600,  ..., -0.0621, -0.0464,  0.0165]],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 0.0652,  0.0485, -0.0705,  0.0223,  0.0040,  0.0065, -0.0599,  0.0454,\n",
            "        -0.0149, -0.0002], requires_grad=True)\n",
            "\n",
            "\n",
            "Layer params:\n",
            "Parameter containing:\n",
            "tensor([[-0.0170,  0.0628,  0.0114,  ...,  0.0605,  0.0552,  0.0457],\n",
            "        [ 0.0609, -0.0412, -0.0646,  ...,  0.0143,  0.0189,  0.0413],\n",
            "        [-0.0168,  0.0396, -0.0511,  ...,  0.0619,  0.0102,  0.0025],\n",
            "        ...,\n",
            "        [-0.0624, -0.0488, -0.0495,  ..., -0.0153,  0.0269,  0.0098],\n",
            "        [-0.0458,  0.0182,  0.0376,  ...,  0.0615,  0.0233,  0.0126],\n",
            "        [-0.0685,  0.0475,  0.0600,  ..., -0.0621, -0.0464,  0.0165]],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 0.0652,  0.0485, -0.0705,  0.0223,  0.0040,  0.0065, -0.0599,  0.0454,\n",
            "        -0.0149, -0.0002], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lin = torch.nn.Linear(3, 2)\n",
        "x = torch.rand(1, 3)\n",
        "print('Input:')\n",
        "print(x)\n",
        "\n",
        "print('\\n\\nWeight and Bias parameters:')\n",
        "for param in lin.parameters():\n",
        "  print(param)\n",
        "\n",
        "y = lin(x)\n",
        "print('\\n\\nOutput:')\n",
        "print(y)"
      ],
      "metadata": {
        "id": "3PCId9mF1WDT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "108ec00f-4ce9-4667-f5cd-1da549c9f13a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            "tensor([[0.0831, 0.0336, 0.1012]])\n",
            "\n",
            "\n",
            "Weight and Bias parameters:\n",
            "Parameter containing:\n",
            "tensor([[ 0.1753,  0.4416, -0.0654],\n",
            "        [ 0.2104, -0.4030,  0.1514]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.5199,  0.4126], requires_grad=True)\n",
            "\n",
            "\n",
            "Output:\n",
            "tensor([[-0.4971,  0.4319]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class LeNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(LeNet, self).__init__()\n",
        "    #1x32x32 input image channel (black & white), 6 output channels, 5x5 convolution kernel\n",
        "    self.conv1 = nn.Conv2d(1, 6, 5)\n",
        "    self.conv2 = nn.Conv2d(6, 16, 3)\n",
        "    #an affine operation: y = Wx + b\n",
        "    self.fc1 = nn.Linear(16 * 6 * 6, 120) #6 * 6 from image dimension\n",
        "    self.fc2 = nn.Linear(120, 84)\n",
        "    self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "      #Max pooling over a (2, 2) window\n",
        "      x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
        "      #if the size is a square you can only specify a single number\n",
        "      x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
        "      x = x.view(-1, self.num_flat_features(x))\n",
        "      x = F.relu(self.fc1(x))\n",
        "      x = F.relu(self.fc2(x))\n",
        "      x = self.fc3(x)\n",
        "      return x\n",
        "\n",
        "    def num_flat_features(self, x):\n",
        "      size = x.size()[1:] #all dimensions except the batch dimension\n",
        "      num_features = 1\n",
        "      for s in size:\n",
        "        num_features *=s\n",
        "        return num_features"
      ],
      "metadata": {
        "id": "MEH5cL1Q5-SE"
      },
      "execution_count": 4,
      "outputs": []
    }
  ]
}